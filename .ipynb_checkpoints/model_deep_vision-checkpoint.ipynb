{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eykdTRywfetw"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4zwn-00qiAZu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/drive/folders/15dFAVS_3xAgxLSq12DZjODVIc10nxvrA?usp=drive_link\""
      ],
      "metadata": {
        "id": "DnSLJVHSl1Yn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo7RWIB8l84x",
        "outputId": "516c7256-48c1-4cd0-e9fe-f53257a206b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_WzkQ1bfmUil"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = '/content/drive/My Drive/images/train'"
      ],
      "metadata": {
        "id": "eFvSUfftnKUR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,       # Normalize pixel values to be between 0 and 1\n",
        "    shear_range=0.2,      # Shear transformations\n",
        "    zoom_range=0.2,       # Zoom transformations\n",
        "    horizontal_flip=True  # Randomly flip images horizontally\n",
        ")\n"
      ],
      "metadata": {
        "id": "WFLxYTwYncrr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_directory ,  # Training data directory\n",
        "    target_size=(224, 224),        # Target size of the images\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'            # or 'categorical' for multi-class classification\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIzx65VenhVw",
        "outputId": "8bc40d47-c096-4fc7-b794-b478fc6a83ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 261 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Create the base ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "num_classes = 7\n",
        "\n",
        "# Add a global average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Add a softmax layer for the number of classes in your dataset\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data generator\n",
        "model.fit(train_generator, epochs=3, steps_per_epoch=len(train_generator), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2q4RNXVnlzO",
        "outputId": "df8d5107-0445-48b9-bead-3c6877a41077"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 204s 19s/step - loss: 2.0461 - accuracy: 0.6820\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 176s 22s/step - loss: 0.9378 - accuracy: 0.7701\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 176s 19s/step - loss: 0.8858 - accuracy: 0.7701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fdfde6cfa60>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def identity_block(X, f, filters):\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, s=2):\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid')(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "\n",
        "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid')(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3)(X_shortcut)\n",
        "\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape=(48, 48, 3), classes=1000):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), padding='same')(X_input)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax')(X)\n",
        "\n",
        "    model = Model(inputs=X_input, outputs=X)\n",
        "\n",
        "    return model\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, epochs=3, steps_per_epoch=len(train_generator), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vty9oTW3ofpN",
        "outputId": "ffe2ab19-be89-4f0c-c420-b3db462a6d3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "9/9 [==============================] - 200s 19s/step - loss: 1.3642 - accuracy: 0.7471\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 177s 19s/step - loss: 1.1194 - accuracy: 0.7625\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 178s 19s/step - loss: 1.0384 - accuracy: 0.7701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fdfda9a33d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class IdentityBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, f, **kwargs):\n",
        "        super(IdentityBlock, self).__init__(**kwargs)\n",
        "        self.F1, self.F2, self.F3 = filters\n",
        "        self.f = f\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv1 = Conv2D(filters=self.F1, kernel_size=(1, 1), strides=(1, 1), padding='valid')\n",
        "        self.bn1 = BatchNormalization(axis=3)\n",
        "        self.act1 = Activation('relu')\n",
        "\n",
        "        self.conv2 = Conv2D(filters=self.F2, kernel_size=(self.f, self.f), strides=(1, 1), padding='same')\n",
        "        self.bn2 = BatchNormalization(axis=3)\n",
        "        self.act2 = Activation('relu')\n",
        "\n",
        "        self.conv3 = Conv2D(filters=self.F3, kernel_size=(1, 1), strides=(1, 1), padding='valid')\n",
        "        self.bn3 = BatchNormalization(axis=3)\n",
        "\n",
        "        self.add = Add()\n",
        "        self.act3 = Activation('relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X_shortcut = inputs\n",
        "\n",
        "        X = self.conv1(inputs)\n",
        "        X = self.bn1(X)\n",
        "        X = self.act1(X)\n",
        "\n",
        "        X = self.conv2(X)\n",
        "        X = self.bn2(X)\n",
        "        X = self.act2(X)\n",
        "\n",
        "        X = self.conv3(X)\n",
        "        X = self.bn3(X)\n",
        "\n",
        "        X = self.add([X, X_shortcut])\n",
        "        X = self.act3(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "class ConvolutionalBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, f, s=2, **kwargs):\n",
        "        super(ConvolutionalBlock, self).__init__(**kwargs)\n",
        "        self.F1, self.F2, self.F3 = filters\n",
        "        self.f = f\n",
        "        self.s = s\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv1 = Conv2D(filters=self.F1, kernel_size=(1, 1), strides=(self.s, self.s), padding='valid')\n",
        "        self.bn1 = BatchNormalization(axis=3)\n",
        "        self.act1 = Activation('relu')\n",
        "\n",
        "        self.conv2 = Conv2D(filters=self.F2, kernel_size=(self.f, self.f), strides=(1, 1), padding='same')\n",
        "        self.bn2 = BatchNormalization(axis=3)\n",
        "        self.act2 = Activation('relu')\n",
        "\n",
        "        self.conv3 = Conv2D(filters=self.F3, kernel_size=(1, 1), strides=(1, 1), padding='valid')\n",
        "        self.bn3 = BatchNormalization(axis=3)\n",
        "\n",
        "        self.conv_shortcut = Conv2D(filters=self.F3, kernel_size=(1, 1), strides=(self.s, self.s), padding='valid')\n",
        "        self.bn_shortcut = BatchNormalization(axis=3)\n",
        "\n",
        "        self.add = Add()\n",
        "        self.act3 = Activation('relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X_shortcut = inputs\n",
        "\n",
        "        X = self.conv1(inputs)\n",
        "        X = self.bn1(X)\n",
        "        X = self.act1(X)\n",
        "\n",
        "        X = self.conv2(X)\n",
        "        X = self.bn2(X)\n",
        "        X = self.act2(X)\n",
        "\n",
        "        X = self.conv3(X)\n",
        "        X = self.bn3(X)\n",
        "\n",
        "        X_shortcut = self.conv_shortcut(X_shortcut)\n",
        "        X_shortcut = self.bn_shortcut(X_shortcut)\n",
        "\n",
        "        X = self.add([X, X_shortcut])\n",
        "        X = self.act3(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "class ResNet48(tf.keras.Model):\n",
        "    def __init__(self, classes=7, **kwargs):\n",
        "        super(ResNet48, self).__init__(**kwargs)\n",
        "        self.classes = classes\n",
        "\n",
        "        self.conv1 = Conv2D(64, (7, 7), strides=(2, 2), padding='same')\n",
        "        self.bn1 = BatchNormalization(axis=3)\n",
        "        self.act1 = Activation('relu')\n",
        "        self.maxpool1 = MaxPooling2D((3, 3), strides=(2, 2))\n",
        "\n",
        "        self.conv_block1 = ConvolutionalBlock([64, 64, 256], 3, s=1)\n",
        "        self.id_block1 = IdentityBlock([64, 64, 256], 3)\n",
        "        self.id_block2 = IdentityBlock([64, 64, 256], 3)\n",
        "\n",
        "        self.conv_block2 = ConvolutionalBlock([128, 128, 512], 3, s=2)\n",
        "        self.id_block3 = IdentityBlock([128, 128, 512], 3)\n",
        "        self.id_block4 = IdentityBlock([128, 128, 512], 3)\n",
        "        self.id_block5 = IdentityBlock([128, 128, 512], 3)\n",
        "\n",
        "        self.conv_block3 = ConvolutionalBlock([256, 256, 1024], 3, s=2)\n",
        "        self.id_block6 = IdentityBlock([256, 256, 1024], 3)\n",
        "        self.id_block7 = IdentityBlock([256, 256, 1024], 3)\n",
        "        self.id_block8 = IdentityBlock([256, 256, 1024], 3)\n",
        "        self.id_block9 = IdentityBlock([256, 256, 1024], 3)\n",
        "        self.id_block10 = IdentityBlock([256, 256, 1024], 3)\n",
        "\n",
        "        self.conv_block4 = ConvolutionalBlock([512, 512, 2048], 3, s=2)\n",
        "        self.id_block11 = IdentityBlock([512, 512, 2048], 3)\n",
        "        self.id_block12 = IdentityBlock([512, 512, 2048], 3)\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        self.dense = Dense(classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X = self.conv1(inputs)\n",
        "        X = self.bn1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.maxpool1(X)\n",
        "\n",
        "        X = self.conv_block1(X)\n",
        "        X = self.id_block1(X)\n",
        "        X = self.id_block2(X)\n",
        "\n",
        "        X = self.conv_block2(X)\n",
        "        X = self.id_block3(X)\n",
        "        X = self.id_block4(X)\n",
        "        X = self.id_block5(X)\n",
        "\n",
        "        X = self.conv_block3(X)\n",
        "        X = self.id_block6(X)\n",
        "        X = self.id_block7(X)\n",
        "        X = self.id_block8(X)\n",
        "        X = self.id_block9(X)\n",
        "        X = self.id_block10(X)\n",
        "\n",
        "        X = self.conv_block4(X)\n",
        "        X = self.id_block11(X)\n",
        "        X = self\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, epochs=3, steps_per_epoch=len(train_generator), verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RCj6Ngxr6C8",
        "outputId": "e0c09f40-da18-4bb6-9265-9be6f865f311"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "9/9 [==============================] - 200s 19s/step - loss: 1.0549 - accuracy: 0.7739\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 176s 21s/step - loss: 0.9771 - accuracy: 0.7701\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 176s 19s/step - loss: 1.2134 - accuracy: 0.7701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fdfded02fb0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_directory = '/content/drive/My Drive/images/validation'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the test images\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_directory,\n",
        "    target_size=(48, 48),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQIJtZ88uqz2",
        "outputId": "5f27d699-98a1-42fb-90fd-6317f11ec9ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 70 images belonging to 7 classes.\n",
            "3/3 [==============================] - 12s 5s/step - loss: 14166.0811 - accuracy: 0.1429\n",
            "Loss: 14166.0810546875, Accuracy: 0.1428571492433548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from flask import Flask, request, jsonify\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the model\n",
        "model = load_model('/content/drive/My Drive/images/model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "PzYfjr2Yy8Sh",
        "outputId": "b2e950be-12a2-433d-d14e-cbe6f48e2d49"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Unable to open file (file signature not found)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7c2967869db6>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/images/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fI-kwPoTz66G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}